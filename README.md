# Sign Language Recognition using LSTM 
*An AI-powered sign language recognition system using LSTM, trained on video datasets, and deployed via a Streamlit app.*

---

## ğŸ“œ Overview

This project is a **Sign Language Recognition System** that leverages a hybrid model of **Long Short-Term Memory (LSTM)**  and **Convolutional Neural Network (CNN)** networks to recognize sign language gestures from video data. The model is trained on a video dataset and presented as a user-friendly **Streamlit web application**. The app also integrates a **speech-to-text** feature and allows users to upload videos for real-time prediction.

---

## ğŸš€ Key Features

- **LSTM Model**: A deep learning model trained on video datasets to recognize temporal patterns in sign language gestures.
- **Convolutional Neural Network (CNN)**: A deep learning model trained to extract spatial features from frames of hand images.
- **Streamlit App**: A web-based interface for users to interact with the model.
- **Speech-to-Text Integration**: Converts spoken language into text for enhanced accessibility.
- **Video Upload Option**: Users can upload videos for real-time sign language recognition.
- **Real-Time Predictions**: The app provides instant predictions for uploaded videos or live input.

---

---

## ï¿½ Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/pratham-asthana/MADHYAM

## ğŸ§  Model Architecture

The LSTM model is designed to process sequential video data and extract spatiotemporal features. Here's a high-level overview of the architecture:

1. **Input Layer** ğŸ¥: Accepts video frames as input.
2. **Preprocessing** ğŸ› ï¸: Frames are resized, normalized, and converted to grayscale.
3. **LSTM Layers** ğŸ”„: Multiple LSTM layers to capture temporal dependencies.
4. **Dense Layers** ğŸ§©: Fully connected layers for classification.
5. **Output Layer** ğŸ¯: Softmax activation for gesture prediction.

---

## ğŸ“Š Dataset

The model is trained on a **videographical dataset** containing sign language gestures. The dataset includes:
- ğŸ“‚ Multiple sign language gestures(different 47 labels across the dataset).
- â© Variations in gesture speed and execution.

*Note: If you're using a custom dataset, ensure it is preprocessed and split into training, validation, and test sets.*

## ğŸš€ Future Improvements

Here are some ideas to make the project even better:

- **Expand Dataset** ğŸ“ˆ: Include more gestures and diverse signers to improve model robustness.
- **Multi-Language Support** ğŸŒ: Add support for multiple sign languages.
- **Model Optimization** âš™ï¸: Experiment with other architectures like Transformers or 3D CNNs.
- **User Feedback Integration** ğŸ’¬: Allow users to provide feedback on predictions to improve the model.

---

## ğŸ¤ Contributing

We welcome contributions! If you'd like to contribute, follow these steps:

1. **Fork the repository** ğŸ´.
2. **Create a new branch** ğŸŒ¿ (`git checkout -b feature/YourFeatureName`).
3. **Commit your changes** ğŸ’¾ (`git commit -m 'Add some feature'`).
4. **Push to the branch** ğŸš€ (`git push origin feature/YourFeatureName`).
5. **Open a pull request** ğŸ›.

---

## ğŸ“§ Contact

For questions, feedback, or collaboration opportunities, feel free to reach out:

- **Pratham Asthana** ğŸ‘¤  
- ğŸ“§ (prathamasthana04@gmail.com)   
- ğŸ’¬ ([https://github.com/pratham-asthana](https://www.linkedin.com/in/pratham-asthana-243133265/))
- **Aryan Verma**
- ğŸ“§ (aryanv090803@gmail.com)
- ğŸ’¬ ([https://github.com/Aryanv-0908](https://www.linkedin.com/in/aryan-verma-aa8a04263/))
- **Neeraj Jha**
- ğŸ“§ (jhaneeraj2003@gmail.com)
- ğŸ’¬ ([https://github.com/neeraj-jhaa](https://www.linkedin.com/in/neeraj-jha-a471521ab/))
- **Akshat**
- ğŸ“§ (akshatlamba4@gmail.com)
- ğŸ’¬ ([https://github.com/akshatlamba1])

---

