# Sign Language Recognition using LSTM 
*An AI-powered sign language recognition system using LSTM, trained on video datasets, and deployed via a Streamlit app.*

---

## 📜 Overview

This project is a **Sign Language Recognition System** that leverages a hybrid model of **Long Short-Term Memory (LSTM)**  and **Convolutional Neural Network (CNN)** networks to recognize sign language gestures from video data. The model is trained on a video dataset and presented as a user-friendly **Streamlit web application**. The app also integrates a **speech-to-text** feature and allows users to upload videos for real-time prediction.

---

## 🚀 Key Features

- **LSTM Model**: A deep learning model trained on video datasets to recognize temporal patterns in sign language gestures.
- **Convolutional Neural Network (CNN)**: A deep learning model trained to extract spatial features from frames of hand images.
- **Streamlit App**: A web-based interface for users to interact with the model.
- **Speech-to-Text Integration**: Converts spoken language into text for enhanced accessibility.
- **Video Upload Option**: Users can upload videos for real-time sign language recognition.
- **Real-Time Predictions**: The app provides instant predictions for uploaded videos or live input.

---

---

## � Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/pratham-asthana/MADHYAM

## 🧠 Model Architecture

The LSTM model is designed to process sequential video data and extract spatiotemporal features. Here's a high-level overview of the architecture:

1. **Input Layer** 🎥: Accepts video frames as input.
2. **Preprocessing** 🛠️: Frames are resized, normalized, and converted to grayscale.
3. **LSTM Layers** 🔄: Multiple LSTM layers to capture temporal dependencies.
4. **Dense Layers** 🧩: Fully connected layers for classification.
5. **Output Layer** 🎯: Softmax activation for gesture prediction.

---

## 📊 Dataset

The model is trained on a **videographical dataset** containing sign language gestures. The dataset includes:
- 📂 Multiple sign language gestures(different 47 labels across the dataset).
- ⏩ Variations in gesture speed and execution.

*Note: If you're using a custom dataset, ensure it is preprocessed and split into training, validation, and test sets.*

## 🚀 Future Improvements

Here are some ideas to make the project even better:

- **Expand Dataset** 📈: Include more gestures and diverse signers to improve model robustness.
- **Multi-Language Support** 🌍: Add support for multiple sign languages.
- **Model Optimization** ⚙️: Experiment with other architectures like Transformers or 3D CNNs.
- **User Feedback Integration** 💬: Allow users to provide feedback on predictions to improve the model.

---

## 🤝 Contributing

We welcome contributions! If you'd like to contribute, follow these steps:

1. **Fork the repository** 🍴.
2. **Create a new branch** 🌿 (`git checkout -b feature/YourFeatureName`).
3. **Commit your changes** 💾 (`git commit -m 'Add some feature'`).
4. **Push to the branch** 🚀 (`git push origin feature/YourFeatureName`).
5. **Open a pull request** 🛎.

---

## 📧 Contact

For questions, feedback, or collaboration opportunities, feel free to reach out:

- **Pratham Asthana** 👤  
- 📧 (prathamasthana04@gmail.com)   
- 💬 ([https://github.com/pratham-asthana](https://www.linkedin.com/in/pratham-asthana-243133265/))
- **Aryan Verma**
- 📧 (aryanv090803@gmail.com)
- 💬 ([https://github.com/Aryanv-0908](https://www.linkedin.com/in/aryan-verma-aa8a04263/))
- **Neeraj Jha**
- 📧 (jhaneeraj2003@gmail.com)
- 💬 ([https://github.com/neeraj-jhaa](https://www.linkedin.com/in/neeraj-jha-a471521ab/))
- **Akshat**
- 📧 (akshatlamba4@gmail.com)
- 💬 ([https://github.com/akshatlamba1])

---

